
# ğŸ§  AI Technical Assignment

## Human & Animal Detection + Offline Industrial OCR

This project implements **two independent AI systems** designed for **offline deployment under real-world industrial constraints**.

* **Part A:** Human & Animal Detection in images and videos
* **Part B:** Offline OCR for industrial / military-style stenciled text

The emphasis of this assignment is on **system design, robustness, explainability, and realistic limitations**, rather than benchmark-driven accuracy.

---

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ detection/              # Part A â€“ Object Detection
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”œâ”€â”€ visualization.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”‚
â”‚   â””â”€â”€ ocr/                    # Part B â€“ Offline OCR
â”‚       â”œâ”€â”€ preprocessing.py
â”‚       â”œâ”€â”€ text_detection.py
â”‚       â”œâ”€â”€ text_recognition.py
â”‚       â”œâ”€â”€ postprocess.py
â”‚       â””â”€â”€ metrics.py
â”‚
â”œâ”€â”€ test_images/                # Part A image inputs
â”œâ”€â”€ test_videos/                # Part A video inputs
â”œâ”€â”€ outputs/                    # Part A outputs + metrics
â”‚
â”œâ”€â”€ ocr_inputs/                 # Part B OCR inputs
â”œâ”€â”€ ocr_outputs/                # Part B OCR outputs + metrics
â”‚
â”œâ”€â”€ main.py                     # Entry point â€“ Part A
â”œâ”€â”€ main_ocr.py                 # Entry point â€“ Part B
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ”¹ Part A â€“ Human & Animal Detection

## ğŸ¯ Objective

Detect and classify **humans and animals** in images/videos using a **robust, offline pipeline** without relying on cloud services.

---

## ğŸ“Š Dataset

**Open Images V7 (Filtered Subset)**

* Chosen for real-world diversity and high-quality annotations
* Classes used:

  * `Person`
  * Animals: Dog, Cat, Horse, Cattle, Sheep, Bird
* Dataset downloaded once using FiftyOne and then used fully offline

---

## ğŸ§  Model

* **Faster R-CNN (ResNet-50 FPN)**
* Pretrained on COCO
* Used strictly in **inference-only mode**
* YOLO explicitly avoided (as per instructions)

### Why Faster R-CNN?

* Strong localization accuracy
* Industry-standard architecture
* Reliable offline performance

---

## ğŸ”„ Detection Pipeline

```
Image / Video
   â†“
Faster R-CNN Detector
   â†“
Class Filtering (Person + Animals only)
   â†“
Semantic Grouping:
   - Person â†’ Human
   - All animals â†’ Animal
   â†“
Annotated Output
```

All videos placed in `test_videos/` are processed automatically.

---

## ğŸ“ˆ Metrics (Part A)

Since the model is not fine-tuned, **training metrics (loss, mAP)** are not recomputed.

Instead, **pipeline-level metrics** are recorded:

* Frames processed
* Average inference time per frame
* FPS
* Average detections per frame

Saved to:

```
outputs/metrics.json
```

Example:

```json
{
  "frames_processed": 49,
  "avg_inference_time_sec": 3.43,
  "fps": 0.29,
  "avg_detections_per_frame": 5.29
}
```

> The low FPS is expected for Faster R-CNN running on CPU and reflects a deliberate trade-off favoring robustness over real-time speed.

---

# ğŸ”¹ Part B â€“ Offline OCR for Industrial / Stenciled Text

## ğŸ¯ Objective

Extract text from **industrial and military-style containers** featuring:

* Stenciled paint
* Low contrast
* Surface wear
* Broken characters

The system must be **fully offline** and output **structured text data**.

---

## ğŸ§  OCR Design Philosophy

Industrial OCR is fundamentally different from document OCR.
This system prioritizes:

* Robustness over accuracy
* Safe failure (no hallucinated text)
* Explainable behavior

---

## ğŸ§° OCR Model

### Text Recognition

* **Microsoft TrOCR (Hugging Face)**
* Model: `microsoft/trocr-base-printed`
* Transformer-based OCR
* Used **locally, offline**
* No fine-tuning performed

### Why TrOCR?

* Significantly more robust than classical OCR on degraded text
* Handles broken glyphs better than Tesseract
* Fully open-source and offline

---

## ğŸ”„ OCR Pipeline

```
Input Image
   â†“
Preprocessing (grayscale, contrast enhancement, thresholding)
   â†“
Stencil-aware text region detection
   â†“
Crop detected regions
   â†“
TrOCR (offline recognition)
   â†“
Post-processing & cleanup
   â†“
Structured JSON output
```

---

## ğŸ“ˆ Metrics (Part B)

OCR performance is evaluated using **pipeline-level metrics**, not character-level accuracy.

Metrics include:

* Number of detected text blocks
* Total inference time

Saved to:

```
ocr_outputs/metrics.json
```

Example:

```json
{
  "text_blocks_detected": 4,
  "total_inference_time_sec": 1.12
}
```

---

## âš ï¸ Known Limitations (Important)

Industrial OCR is inherently difficult.

This system may **intentionally return no OCR output** for images where:

* Contrast is extremely low
* Stencil paint is heavily worn
* Text blends into the background

This behavior is **by design** to avoid false positives.

> Returning no text is preferable to hallucinating incorrect text in industrial systems.

---

## â–¶ï¸ How to Run

### Part A â€“ Detection

```bash
python main.py
```

### Part B â€“ OCR

```bash
python main.py
```

---

## ğŸ“¤ Outputs

### Part A

* Annotated images/videos â†’ `outputs/`
* Metrics â†’ `outputs/metrics.json`

### Part B

* Annotated image â†’ `ocr_outputs/annotated.jpg`
* Extracted text â†’ `ocr_outputs/result.json`
* Metrics â†’ `ocr_outputs/metrics.json`

